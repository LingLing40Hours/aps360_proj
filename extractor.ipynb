{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchtext.data\n",
    "import spacy\n",
    "from torchtext.data import Field, RawField, BucketIterator, Example, Dataset\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x107cc9af0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filepaths\n",
    "clean_img_path = 'clean_img'\n",
    "clean_txt_path = 'clean_txt'\n",
    "clean_entity_path = 'clean_entity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect all records into a list\n",
    "records = []\n",
    "clean_imgs = torch.load(os.path.join(clean_img_path, 'clean.pt'))\n",
    "\n",
    "for i, name in enumerate(os.listdir(clean_txt_path)):\n",
    "    name, __ = os.path.splitext(name)\n",
    "    img = clean_imgs[i]\n",
    "    txt_filepath = os.path.join(clean_txt_path, name + '.txt')\n",
    "    txt_file = open(txt_filepath, 'rb')\n",
    "    text = txt_file.read().decode('latin1')\n",
    "    txt_file.close()\n",
    "    ent_filepath = os.path.join(clean_entity_path, name + '.pkl')\n",
    "    #print(ent_filepath)\n",
    "    #ent_file = open(ent_filepath, 'rb')\n",
    "    try:\n",
    "        with open(ent_filepath, 'rb') as my_file:\n",
    "            unpickler = pickle.Unpickler(my_file)\n",
    "            entities = unpickler.load()\n",
    "    except EOFError:\n",
    "        print(ent_filepath)\n",
    "        print('An EOFError exception occurred. The file is empty')\n",
    "    #entities = pickle.load(ent_file)\n",
    "    #record = [img, text, entities]\n",
    "    record = [text, entities]\n",
    "    records.append(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set len: 681\n",
      "Validation set len: 146\n",
      "Test set len: 146\n"
     ]
    }
   ],
   "source": [
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "train_data, temp_data = train_test_split(records, test_size=(1 - train_ratio))\n",
    "val_data, test_data = train_test_split(temp_data, test_size=test_ratio / (val_ratio + test_ratio))\n",
    "print(\"Training set len:\", len(train_data))\n",
    "print(\"Validation set len:\", len(val_data))\n",
    "print(\"Test set len:\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"3180303 LIAN HING STATIONERY SDN BHD (162761-M) NO.32 & 33,JALAN SR 1/9. SEKSYEN 9. TAMAN SERDANG RAYA, 43300 SERI KEMBANGAN, SELANGOR DARUL EHSAN GST ID : 002139201536 TAX INVOICE 27/03/2018 NO.: CS-20242 QTY TAX RM DURSFILE H399(110 X 95MM) 100 SR 58.30 NAME BADGE (H) @ 0.5500 809 METAL NAME BADGE CLIP 1 SR 21.20 100'S @ 20.0000 TOTAL AMT INCL. GST @ 6%: 79.50 ROUNDING ADJUSTMENT: TOTAL AMT PAYABLE: 79.50 PAID AMOUNT: 100.00 CHANGE: 20.50 TOTAL QTY TENDER: 101 GST SUMMARY AMOUNT TAX (RM) (RM) SR @ A 75.00 4.50 TOTAL 75.00 4.50 THANK YOU FOR ANY ENQUIRY, PLEASE CONTACT US;\",\n",
       " ['LIAN HING STATIONERY SDN BHD',\n",
       "  '27/03/2018',\n",
       "  'NO.32 & 33, JALAN SR 1/9, SEKSYEN 9, TAMAN SERDANG RAYA, 43300 SERI KEMBANGAN, SELANGOR DARUL EHSAN',\n",
       "  '79.50']]"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_vocab = [' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',\n",
    "              'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
    "              '`', '1', '2', '3', '4', '5', '6', '7', '8', '9', '0', '-', '=', '~', '!', '@', '#', '$', '%', '^', '&', '*', '(', ')', '_', '+',\n",
    "              '[', ']', '\\\\', ';', '\\'', ',', '.', '?', '{', '}', '|', ':', '\\\"', '<', '>', '?', '/', 'Â', '·']\n",
    "\n",
    "def index_to_char(i):\n",
    "    return char_vocab[i]\n",
    "\n",
    "def char_to_index(c):\n",
    "    return char_vocab.index(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "#probabilities of [start name, end name, start date, end date, start address, end address, start total, end total]\n",
    "#return tensor of ground truth probabilities; end index is inclusive\n",
    "def get_ground_probs(text, ents, length):\n",
    "    name_start = torch.zeros(length)\n",
    "    name_end = torch.zeros(length)\n",
    "    date_start = torch.zeros(length)\n",
    "    date_end = torch.zeros(length)\n",
    "    address_start = torch.zeros(length)\n",
    "    address_end = torch.zeros(length)\n",
    "    total_start = torch.zeros(length)\n",
    "    total_end = torch.zeros(length)\n",
    "    name_start_index = text.index(ents[0])\n",
    "    name_end_index = name_start_index + len(ents[0]) - 1\n",
    "    date_start_index = text.index(ents[1])\n",
    "    date_end_index = date_start_index + len(ents[1]) - 1\n",
    "    if text.find(ents[2]) == -1:\n",
    "        print(\"text: \", text)\n",
    "        print(\"addr: \", ents[2])\n",
    "    address_start_index = text.index(ents[2])\n",
    "    address_end_index = date_start_index + len(ents[2]) - 1\n",
    "    total_start_index = text.index(ents[3])\n",
    "    total_end_index = total_start_index + len(ents[3]) - 1\n",
    "    name_start[name_start_index] = 1\n",
    "    name_end[name_end_index] = 1\n",
    "    date_start[date_start_index] = 1\n",
    "    date_end[date_end_index] = 1\n",
    "    address_start[address_start_index] = 1\n",
    "    address_end[address_end_index] = 1\n",
    "    total_start[total_start_index] = 1\n",
    "    total_end[total_end_index] = 1\n",
    "    return torch.stack([name_start, name_end, date_start, date_end, address_start, address_end, total_start, total_end])\n",
    "\n",
    "def get_batch_ground_probs(batch_text, batch_ents): #[batch, 8*char]\n",
    "    max_len = len(batch_text[-1])\n",
    "    ans = []\n",
    "    for text, ents in zip(batch_text, batch_ents):\n",
    "        assert(len(text) > 0)\n",
    "        ans.append(get_ground_probs(text, ents, max_len))\n",
    "    return torch.stack(ans).view(batch_text.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_onehot(text, length):\n",
    "    ident = torch.eye(len(char_vocab))\n",
    "    ans = []\n",
    "    for char in text:\n",
    "        ans.append(ident[char_to_index(char)])\n",
    "    for i in range(length - len(text)):\n",
    "        ans.append(ident[char_to_index(' ')])\n",
    "    assert(len(ans) == length)\n",
    "    return torch.stack(ans)\n",
    "\n",
    "def get_batch_numerical(batch_data):\n",
    "    #for data in batch_data:\n",
    "    #    assert(len(data[0]) > 0)\n",
    "    batch_text = [data[0] for data in batch_data]\n",
    "    batch_ents = [data[1] for data in batch_data]\n",
    "    max_len = len(batch_text[-1])\n",
    "    batch_enc_text = torch.stack([text_to_onehot(text, max_len) for text in batch_text])\n",
    "    batch_probs = get_batch_ground_probs(batch_text, batch_ents)\n",
    "    return batch_enc_text, batch_probs\n",
    "\n",
    "def get_data_loader(data, batch_size):\n",
    "    #for d in data:\n",
    "    #    assert(len(d[0]) > 0)\n",
    "    sorted_data = sorted(data, key=lambda x: len(x[0]))\n",
    "    ans = []\n",
    "    for i in range(0, len(sorted_data), batch_size):\n",
    "        batch_data = sorted_data[i:i+batch_size]\n",
    "        ans.append(get_batch_numerical(batch_data))\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EXT(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EXT, self).__init__()\n",
    "        self.emb = nn.Embedding(input_size, input_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        h0 = torch.zeros(1, len(x), self.hidden_size)  # Initial hidden state\n",
    "        out, _ = self.rnn(x, h0) #[batch index, char index, vocab index]\n",
    "        out = self.fc(out) #[batch index, char index, 8]\n",
    "        out = torch.transpose(out, 1, 2) #[batch index, 8, char index]\n",
    "        out = F.softmax(out, dim=2).view(out.size(0), -1) #[batch, 8*char]\n",
    "        return out #The output_probs tensor contains probabilities for each class at each position in the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.linear_q = nn.Linear(input_size, hidden_size)\n",
    "        self.linear_k = nn.Linear(input_size, hidden_size)\n",
    "        self.linear_v = nn.Linear(input_size, hidden_size)\n",
    "        self.linear_x = nn.Linear(input_size, hidden_size)\n",
    "        self.attention = nn.MultiheadAttention(hidden_size, num_heads=4, batch_first=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size)\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        q, k, v = self.linear_q(x), self.linear_k(x), self.linear_v(x)\n",
    "        x = self.norm(self.linear_x(x) + self.attention(q, k, v))\n",
    "        x = self.norm(x + self.fc(x))\n",
    "        return x\n",
    "\n",
    "class EXT2(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EXT2, self).__init__()\n",
    "        self.emb = nn.Embedding(input_size, input_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.transformer = TransformerEncoder(input_size, hidden_size)  # Include TransformerEncoder here\n",
    "        self.fc = nn.Linear(hidden_size, 8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        h0 = torch.zeros(1, len(x), self.hidden_size)  # Initial hidden state\n",
    "        out_rnn, _ = self.rnn(x, h0)  # GRU output\n",
    "        out_transformer = self.transformer(x)  # Transformer output\n",
    "        out_combined = out_rnn + out_transformer  # Combine outputs\n",
    "        out = self.fc(out_combined)  # Apply linear layer\n",
    "        out = torch.transpose(out, 1, 2) #[batch index, 8, char index]\n",
    "        out = F.softmax(out, dim=2).view(out.size(0), -1) #[batch, 8*char]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cer(reference, hypothesis):\n",
    "    # Convert the sentences into character lists\n",
    "    ref = list(reference)\n",
    "    hyp = list(hypothesis)\n",
    "\n",
    "    # Create a matrix of size (len(ref)+1) x (len(hyp)+1)\n",
    "    d = np.zeros((len(ref) + 1) * (len(hyp) + 1), dtype=np.uint8)\n",
    "    d = d.reshape((len(ref) + 1, len(hyp) + 1))\n",
    "\n",
    "    # Initialize the first row and column to be the distance from the empty string\n",
    "    for i in range(len(ref) + 1):\n",
    "        d[i][0] = i\n",
    "    for j in range(len(hyp) + 1):\n",
    "        d[0][j] = j\n",
    "\n",
    "    # Populate the rest of the matrix\n",
    "    for i in range(1, len(ref) + 1):\n",
    "        for j in range(1, len(hyp) + 1):\n",
    "            if ref[i - 1] == hyp[j - 1]:\n",
    "                cost = 0\n",
    "            else:\n",
    "                cost = 1\n",
    "            d[i][j] = min(d[i - 1][j] + 1,      # deletion\n",
    "                          d[i][j - 1] + 1,      # insertion\n",
    "                          d[i - 1][j - 1] + cost)  # substitution\n",
    "\n",
    "    # The CER is the cost of transforming hypothesis into reference divided by the number of characters in the reference\n",
    "    cer_value = float(d[len(ref)][len(hyp)]) / len(ref)\n",
    "\n",
    "    return cer_value\n",
    "\n",
    "def car(reference, hypothesis):\n",
    "    return 1 - cer(reference, hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_name(name, batch_size, learning_rate, epoch):\n",
    "    return \"model_{}_bs{}_lr{}_epoch{}\".format(name, batch_size, learning_rate, epoch)\n",
    "\n",
    "#based on CER\n",
    "def get_accuracy(model, data_itr):\n",
    "    total = 0\n",
    "    total_car = 0 #sum of char acc rate over all samples\n",
    "    total_ear = 0 #sum of entity acc rate over all samples\n",
    "    total_corr = 0 #sum of sample acc rate over all samples\n",
    "    for batch_text, batch_ents in data_itr:\n",
    "        batch_output = model(batch_text).view(batch_output.size(0), 8, -1) #[batch, 8, char]\n",
    "        batch_output = torch.argmax(batch_output, dim=2) #[batch, 8]\n",
    "\n",
    "        pred_names = [text[output[0]:output[1]+1] for text, output in zip(batch_text, batch_output)] #+1 since string slicing is exclusive\n",
    "        pred_dates = [text[output[2]:output[3]+1] for text, output in zip(batch_text, batch_output)]\n",
    "        pred_addresses = [text[output[4]:output[5]+1] for text, output in zip(batch_text, batch_output)]\n",
    "        pred_totals = [text[output[6]:output[7]+1] for text, output in zip(batch_text, batch_output)]\n",
    "\n",
    "        total += len(batch_text)\n",
    "        total_car += sum([car(ents[0]+ents[1]+ents[2]+ents[3], pred_name+pred_date+pred_address+pred_total)\n",
    "                          for ents, pred_name, pred_date, pred_address, pred_total in\n",
    "                          zip(batch_ents, pred_names, pred_dates, pred_addresses, pred_totals)])\n",
    "        total_ear += sum([(int(ents[0]==pred_name) + int(ents[1]==pred_date) + int(ents[2]==pred_address) + int(ents[3]==pred_total))/4.0\n",
    "                          for ents, pred_name, pred_date, pred_address, pred_total in\n",
    "                          zip(batch_ents, pred_names, pred_dates, pred_addresses, pred_totals)\n",
    "        ])\n",
    "        total_corr += sum([int(ents[0]+ents[1]+ents[2]+ents[3] == pred_name+pred_date+pred_address+pred_total)\n",
    "                          for ents, pred_name, pred_date, pred_address, pred_total in\n",
    "                          zip(batch_ents, pred_names, pred_dates, pred_addresses, pred_totals)\n",
    "        ])\n",
    "    char_acc = total_car / total\n",
    "    ent_acc = total_ear / total\n",
    "    sample_acc = total_corr / total\n",
    "    return char_acc, ent_acc, sample_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(model, data_itr, criterion):\n",
    "    loss = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_text, batch_ents in data_itr:\n",
    "            batch_output = model(batch_text)\n",
    "            ground_probs = get_batch_ground_probs(batch_text, batch_ents) #[batch, 8*char]\n",
    "            loss += criterion(batch_output, ground_probs)\n",
    "            total += batch_output.size(0)\n",
    "    return loss / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_set, val_set, batch_size=32, num_epochs=5, learning_rate=1e-5):\n",
    "    # DataLoaders for train and validation sets\n",
    "    # Create BucketIterator\n",
    "    train_itr = get_data_loader(train_set, batch_size)\n",
    "    val_itr = get_data_loader(val_set, batch_size)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    train_loss, val_loss, train_acc, valid_acc = [], [], [], []\n",
    "    epochs = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (batch_text, batch_ents) in enumerate(train_itr):\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(batch_text)\n",
    "            loss = criterion(pred, get_batch_ground_probs(batch_text, batch_ents))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss.append(get_loss(model, train_itr, criterion))\n",
    "        val_loss.append(get_loss(model, val_itr, criterion))\n",
    "\n",
    "        epochs.append(epoch)\n",
    "        train_acc.append(get_accuracy(model, train_itr))\n",
    "        valid_acc.append(get_accuracy(model, val_itr))\n",
    "        print(\"Epoch %d; Train Loss %f; Val Loss %f Train Acc %f; Val Acc %f\" % (\n",
    "            epoch+1, train_loss[-1], val_loss[-1], train_acc[-1], valid_acc[-1]))\n",
    "\n",
    "    # plotting\n",
    "    plt.title(\"Training Curve\")\n",
    "    plt.plot(epochs, train_loss, label=\"Train\")\n",
    "    plt.plot(epochs, val_loss, label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.title(\"Training Curve\")\n",
    "    plt.plot(epochs, train_acc, label=\"Train\")\n",
    "    plt.plot(epochs, valid_acc, label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  RESTAURANT SIN DU K3-113,JL IBRAHIM SULTAN 80300 JOHOR BAHRU JOHOR H/P: 019-7521215 016-7867868 09/03/2018 21:28 0001 000000#7259 CASHIER01 DPT.05 RM 149.00 DPT.04 RM 21.00 CASH RM 170.00\n",
      "addr:  K3-113, JL IBRAHIM SULTAN 80300 JOHOR BAHRU JOHOR\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "substring not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb Cell 16\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m george \u001b[39m=\u001b[39m EXT(\u001b[39mlen\u001b[39m(char_vocab), \u001b[39m8\u001b[39m\u001b[39m*\u001b[39m\u001b[39mlen\u001b[39m(char_vocab))\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m train_model(george, train_data, val_data)\n",
      "\u001b[1;32m/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb Cell 16\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_model\u001b[39m(model, train_set, val_set, batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, num_epochs\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, learning_rate\u001b[39m=\u001b[39m\u001b[39m1e-5\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m# DataLoaders for train and validation sets\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m# Create BucketIterator\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     train_itr \u001b[39m=\u001b[39m get_data_loader(train_set, batch_size)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     val_itr \u001b[39m=\u001b[39m get_data_loader(val_set, batch_size)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()\n",
      "\u001b[1;32m/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb Cell 16\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb#X13sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(sorted_data), batch_size):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb#X13sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     batch_data \u001b[39m=\u001b[39m sorted_data[i:i\u001b[39m+\u001b[39mbatch_size]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb#X13sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     ans\u001b[39m.\u001b[39mappend(get_batch_numerical(batch_data))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb#X13sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mreturn\u001b[39;00m ans\n",
      "\u001b[1;32m/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb#X13sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m max_len \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(batch_text[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb#X13sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m batch_enc_text \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([text_to_onehot(text, max_len) \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m batch_text])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb#X13sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m batch_probs \u001b[39m=\u001b[39m get_batch_ground_probs(batch_text, batch_ents)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb#X13sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mreturn\u001b[39;00m batch_enc_text, batch_probs\n",
      "\u001b[1;32m/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb Cell 16\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb#X13sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mfor\u001b[39;00m text, ents \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(batch_text, batch_ents):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb#X13sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     \u001b[39massert\u001b[39;00m(\u001b[39mlen\u001b[39m(text) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb#X13sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     ans\u001b[39m.\u001b[39mappend(get_ground_probs(text, ents, max_len))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb#X13sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mstack(ans)\u001b[39m.\u001b[39mview(batch_text\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;32m/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb#X13sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mtext: \u001b[39m\u001b[39m\"\u001b[39m, text)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb#X13sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39maddr: \u001b[39m\u001b[39m\"\u001b[39m, ents[\u001b[39m2\u001b[39m])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb#X13sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m address_start_index \u001b[39m=\u001b[39m text\u001b[39m.\u001b[39;49mindex(ents[\u001b[39m2\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb#X13sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m address_end_index \u001b[39m=\u001b[39m date_start_index \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(ents[\u001b[39m2\u001b[39m]) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb#X13sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m total_start_index \u001b[39m=\u001b[39m text\u001b[39m.\u001b[39mindex(ents[\u001b[39m3\u001b[39m])\n",
      "\u001b[0;31mValueError\u001b[0m: substring not found"
     ]
    }
   ],
   "source": [
    "george = EXT(len(char_vocab), 8*len(char_vocab))\n",
    "\n",
    "train_model(george, train_data, val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
