{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchtext.data\n",
    "import spacy\n",
    "from torchtext.data import Field, RawField, BucketIterator, Example, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from torchmetrics.text import CharErrorRate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1094d1ad0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filepaths\n",
    "clean_img_path = 'clean_img'\n",
    "clean_txt_path = 'clean_txt'\n",
    "clean_entity_path = 'clean_entity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect all records into a list\n",
    "records = []\n",
    "clean_imgs = torch.load(os.path.join(clean_img_path, 'clean.pt'))\n",
    "\n",
    "for i, name in enumerate(os.listdir(clean_txt_path)):\n",
    "    name, __ = os.path.splitext(name)\n",
    "    img = clean_imgs[i]\n",
    "    txt_filepath = os.path.join(clean_txt_path, name + '.txt')\n",
    "    txt_file = open(txt_filepath, 'rb')\n",
    "    text = txt_file.read().decode('latin1')\n",
    "    txt_file.close()\n",
    "    ent_filepath = os.path.join(clean_entity_path, name + '.pkl')\n",
    "    #print(ent_filepath)\n",
    "    #ent_file = open(ent_filepath, 'rb')\n",
    "    try:\n",
    "        with open(ent_filepath, 'rb') as my_file:\n",
    "            unpickler = pickle.Unpickler(my_file)\n",
    "            entities = unpickler.load()\n",
    "    except EOFError:\n",
    "        print(ent_filepath)\n",
    "        print('An EOFError exception occurred. The file is empty')\n",
    "    #entities = pickle.load(ent_file)\n",
    "    #record = [img, text, entities]\n",
    "    record = [text, entities]\n",
    "    records.append(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set len: 681\n",
      "Validation set len: 146\n",
      "Test set len: 146\n"
     ]
    }
   ],
   "source": [
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "train_data, temp_data = train_test_split(records, test_size=(1 - train_ratio))\n",
    "val_data, test_data = train_test_split(temp_data, test_size=test_ratio / (val_ratio + test_ratio))\n",
    "print(\"Training set len:\", len(train_data))\n",
    "print(\"Validation set len:\", len(val_data))\n",
    "print(\"Test set len:\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['POTTERS GARDEN SDN BHD (1153774-D) BATU 11 . SG BULOH , 47000 SELANGOR . TEL : 016-667 0982 , 016-333 3812 GST REG NO : 000392024064 TAX INVOICE TABLE : 01 POS ID : G INV NO : G/00002268 CASHIER: ADMINISTRATOR INV DT: 04/01/2018 12:29:18 PM RM ITEM AMOUNT AA00007 PLASTIC PLATE AA00022 CHINA POT 2 @7.90 AA00016 PLANT AA00016 PLANT 5 SUB TOTAL 1.00 15.80 16.00 7.00 39.80 SR SR SR SR NET TOTAL CASH CHANGE 39.80 40.00 0.20 TAX SUMMARY SR INCLUSIVE GST 6% AMOUNT 37.54 TAX 2.26 *** THANK YOU PLEASE COME AGAIN *** ITEM SOLD ARE NOT REFUNDABLE & EXCHANGEABLE',\n",
       " ['POTTERS GARDEN SDN BHD',\n",
       "  '04/01/2018',\n",
       "  'BATU 11 . SG BULOH , 47000 SELANGOR .',\n",
       "  '39.80']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_vocab = [' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',\n",
    "              'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
    "              '`', '1', '2', '3', '4', '5', '6', '7', '8', '9', '0', '-', '=', '~', '!', '@', '#', '$', '%', '^', '&', '*', '(', ')', '_', '+',\n",
    "              '[', ']', '\\\\', ';', '\\'', ',', '.', '?', '{', '}', '|', ':', '\\\"', '<', '>', '?', '/', 'Â', '·', 'Ã', '\\x83', '\\x82']\n",
    "\n",
    "def index_to_char(i):\n",
    "    return char_vocab[i]\n",
    "\n",
    "def char_to_index(c):\n",
    "    return char_vocab.index(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_space_count(s):\n",
    "    ans = 0\n",
    "    for c in s:\n",
    "        if c == ' ':\n",
    "            ans += 1\n",
    "        else:\n",
    "            break\n",
    "    return ans\n",
    "\n",
    "def get_index_ignoring_spaces(main_string, substring):\n",
    "    # Remove spaces from both main string and substring for comparison\n",
    "    main_string_no_spaces = main_string.replace(\" \", \"\")\n",
    "    substring_no_spaces = substring.replace(\" \", \"\")\n",
    "\n",
    "    # Get the index where the substring (ignoring spaces) occurs in the modified main string\n",
    "    index = main_string_no_spaces.find(substring_no_spaces)\n",
    "    end_index = index + len(substring_no_spaces)\n",
    "\n",
    "    #print(\"index: \", index)\n",
    "    #print(\"end index: \", end_index)\n",
    "\n",
    "    if index != -1:\n",
    "        # Calculate the adjusted index considering spaces in the original string\n",
    "        non_space_count = 0\n",
    "        adjusted_index = 0\n",
    "        adjusted_end_index = 0\n",
    "        for i in range(len(main_string)):\n",
    "            if non_space_count < index:\n",
    "                adjusted_index += 1;\n",
    "            else:\n",
    "                break\n",
    "            if main_string[i] != ' ':\n",
    "                non_space_count += 1;\n",
    "        non_space_count = 0\n",
    "        for i in range(len(main_string)):\n",
    "            if non_space_count < end_index:\n",
    "                adjusted_end_index += 1;\n",
    "            else:\n",
    "                break\n",
    "            if main_string[i] != ' ':\n",
    "                non_space_count += 1;\n",
    "        adjusted_index += initial_space_count(main_string[adjusted_index:])\n",
    "        return adjusted_index, adjusted_end_index\n",
    "    else:\n",
    "        return -1, -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#probabilities of [start name, end name, start date, end date, start address, end address, start total, end total]\n",
    "#return tensor of ground truth probabilities; end index is inclusive\n",
    "def get_ground_probs(text, ents, length):\n",
    "    name_start = torch.zeros(length)\n",
    "    name_end = torch.zeros(length)\n",
    "    date_start = torch.zeros(length)\n",
    "    date_end = torch.zeros(length)\n",
    "    address_start = torch.zeros(length)\n",
    "    address_end = torch.zeros(length)\n",
    "    total_start = torch.zeros(length)\n",
    "    total_end = torch.zeros(length)\n",
    "    name_start_index, name_end_index = get_index_ignoring_spaces(text, ents[0])\n",
    "    name_end_index -= 1\n",
    "    date_start_index, date_end_index = get_index_ignoring_spaces(text, ents[1])\n",
    "    date_end_index -= 1\n",
    "    address_start_index, address_end_index = get_index_ignoring_spaces(text, ents[2])\n",
    "    address_end_index -= 1\n",
    "    total_start_index, total_end_index = get_index_ignoring_spaces(text, ents[3])\n",
    "    total_end_index -= 1\n",
    "    name_start[name_start_index] = 1\n",
    "    name_end[name_end_index] = 1\n",
    "    date_start[date_start_index] = 1\n",
    "    date_end[date_end_index] = 1\n",
    "    address_start[address_start_index] = 1\n",
    "    if text.find(ents[2]) == -1:\n",
    "        print(\"text: \", text)\n",
    "        print(\"addr: \", ents[2])\n",
    "    address_end[address_end_index] = 1\n",
    "    total_start[total_start_index] = 1\n",
    "    total_end[total_end_index] = 1\n",
    "    return torch.stack([name_start, name_end, date_start, date_end, address_start, address_end, total_start, total_end])\n",
    "\n",
    "def get_batch_ground_probs(batch_text, batch_ents): #[batch, 8*char]\n",
    "    max_len = len(batch_text[-1])\n",
    "    ans = []\n",
    "    for text, ents in zip(batch_text, batch_ents):\n",
    "        assert(len(text) > 0)\n",
    "        ans.append(get_ground_probs(text, ents, max_len))\n",
    "    return torch.stack(ans).view(len(batch_text), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_onehot(text, length):\n",
    "    ident = torch.eye(len(char_vocab))\n",
    "    ans = []\n",
    "    for char in text:\n",
    "        ans.append(ident[char_to_index(char)])\n",
    "    for i in range(length - len(text)):\n",
    "        ans.append(ident[char_to_index(' ')])\n",
    "    assert(len(ans) == length)\n",
    "    return torch.stack(ans)\n",
    "\n",
    "def get_batch_numerical(batch_data):\n",
    "    #for data in batch_data:\n",
    "    #    assert(len(data[0]) > 0)\n",
    "    batch_text = [data[0] for data in batch_data]\n",
    "    batch_ents = [data[1] for data in batch_data]\n",
    "    max_len = len(batch_text[-1])\n",
    "    batch_enc_text = torch.stack([text_to_onehot(text, max_len) for text in batch_text])\n",
    "    batch_probs = get_batch_ground_probs(batch_text, batch_ents)\n",
    "    return batch_enc_text, batch_probs\n",
    "\n",
    "def get_data_loader(data, batch_size):\n",
    "    #for d in data:\n",
    "    #    assert(len(d[0]) > 0)\n",
    "    sorted_data = sorted(data, key=lambda x: len(x[0]))\n",
    "    ans = []\n",
    "    for i in range(0, len(sorted_data), batch_size):\n",
    "        batch_data = sorted_data[i:i+batch_size]\n",
    "        batch_text = [d[0] for d in batch_data]\n",
    "        batch_ents = [d[1] for d in batch_data]\n",
    "        batch_enc_text, ground_probs = get_batch_numerical(batch_data)\n",
    "        ans.append((batch_enc_text, ground_probs, batch_text, batch_ents))\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EXT(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EXT, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        assert(torch.is_tensor(x))\n",
    "        h0 = torch.zeros(1, len(x), self.hidden_size)  # Initial hidden state\n",
    "        out, _ = self.rnn(x, h0) #[batch index, char index, vocab index]\n",
    "        out = self.fc(out) #[batch index, char index, 8]\n",
    "        out = torch.transpose(out, 1, 2) #[batch index, 8, char index]\n",
    "        out = F.softmax(out, dim=2).view(out.size(0), -1) #[batch, 8*char]\n",
    "        return out #The output_probs tensor contains probabilities for each class at each position in the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.linear_q = nn.Linear(input_size, hidden_size)\n",
    "        self.linear_k = nn.Linear(input_size, hidden_size)\n",
    "        self.linear_v = nn.Linear(input_size, hidden_size)\n",
    "        self.linear_x = nn.Linear(input_size, hidden_size)\n",
    "        self.attention = nn.MultiheadAttention(hidden_size, num_heads=4, batch_first=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size)\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        q, k, v = self.linear_q(x), self.linear_k(x), self.linear_v(x)\n",
    "        temp = self.attention(q, k, v)\n",
    "        x = self.norm(self.linear_x(x) + temp[0])\n",
    "        x = self.norm(x + self.fc(x))\n",
    "        return x\n",
    "\n",
    "class EXT2(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EXT2, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.transformer = TransformerEncoder(input_size, hidden_size)  # Include TransformerEncoder here\n",
    "        self.fc = nn.Linear(hidden_size, 8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1, len(x), self.hidden_size, device=x.device)  # Initial hidden state\n",
    "        out_rnn, _ = self.rnn(x, h0)  # GRU output\n",
    "        out_transformer = self.transformer(x)  # Transformer output\n",
    "        out_combined = out_rnn + out_transformer  # Combine outputs\n",
    "        out = self.fc(out_combined)  # Apply linear layer\n",
    "        out = torch.transpose(out, 1, 2) #[batch index, 8, char index]\n",
    "        out = F.softmax(out, dim=2).view(out.size(0), -1) #[batch, 8*char]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cer = CharErrorRate()\n",
    "\n",
    "def car(reference, hypothesis):\n",
    "    return 1 - cer(reference, hypothesis).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_name(name, batch_size, learning_rate, epoch):\n",
    "    return \"model_{}_bs{}_lr{}_epoch{}\".format(name, batch_size, learning_rate, epoch)\n",
    "\n",
    "#based on CER\n",
    "def get_accuracy(model, data_itr):\n",
    "    total = 0\n",
    "    total_car = 0 #sum of char acc rate over all samples\n",
    "    total_ear = 0 #sum of entity acc rate over all samples\n",
    "    total_corr = 0 #sum of sample acc rate over all samples\n",
    "    for batch_enc_text, ground_probs, batch_text, batch_ents in data_itr:\n",
    "        batch_output = model(batch_enc_text).view(len(batch_enc_text), 8, -1) #[batch, 8, char]\n",
    "        batch_output = torch.argmax(batch_output, dim=2) #[batch, 8]\n",
    "\n",
    "        pred_names = [text[output[0]:output[1]+1] for text, output in zip(batch_text, batch_output)] #+1 since string slicing is exclusive\n",
    "        pred_dates = [text[output[2]:output[3]+1] for text, output in zip(batch_text, batch_output)]\n",
    "        pred_addresses = [text[output[4]:output[5]+1] for text, output in zip(batch_text, batch_output)]\n",
    "        pred_totals = [text[output[6]:output[7]+1] for text, output in zip(batch_text, batch_output)]\n",
    "\n",
    "        total += len(batch_text)\n",
    "        total_car += sum([car(ents[0]+ents[1]+ents[2]+ents[3], pred_name+pred_date+pred_address+pred_total)\n",
    "                          for ents, pred_name, pred_date, pred_address, pred_total in\n",
    "                          zip(batch_ents, pred_names, pred_dates, pred_addresses, pred_totals)])\n",
    "        total_ear += sum([(int(ents[0]==pred_name) + int(ents[1]==pred_date) + int(ents[2]==pred_address) + int(ents[3]==pred_total))/4.0\n",
    "                          for ents, pred_name, pred_date, pred_address, pred_total in\n",
    "                          zip(batch_ents, pred_names, pred_dates, pred_addresses, pred_totals)\n",
    "        ])\n",
    "        total_corr += sum([int(ents[0]+ents[1]+ents[2]+ents[3] == pred_name+pred_date+pred_address+pred_total)\n",
    "                          for ents, pred_name, pred_date, pred_address, pred_total in\n",
    "                          zip(batch_ents, pred_names, pred_dates, pred_addresses, pred_totals)\n",
    "        ])\n",
    "    char_acc = total_car / total\n",
    "    ent_acc = total_ear / total\n",
    "    sample_acc = total_corr / total\n",
    "    return char_acc, ent_acc, sample_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(model, text):\n",
    "    enc_text = text_to_onehot(text, len(text)).unsqueeze(0)\n",
    "    output = model(enc_text).view(8, -1)\n",
    "    output = torch.argmax(output, dim=1).squeeze(0) #[8]\n",
    "\n",
    "    pred_name = text[output[0]:output[1]+1]\n",
    "    pred_date = text[output[2]:output[3]+1]\n",
    "    pred_address = text[output[4]:output[5]+1]\n",
    "    pred_total = text[output[6]:output[7]+1]\n",
    "    return pred_name, pred_date, pred_address, pred_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(model, data_itr, criterion):\n",
    "    loss = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_enc_text, ground_probs, batch_text, batch_ents in data_itr:\n",
    "            batch_output = model(batch_enc_text)\n",
    "            loss += criterion(batch_output, ground_probs)\n",
    "            total += batch_output.size(0)\n",
    "    return loss / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, model_name, train_set, val_set, batch_size=32, num_epochs=5, learning_rate=1e-5):\n",
    "    # DataLoaders for train and validation sets\n",
    "    # Create BucketIterator\n",
    "    train_itr = get_data_loader(train_set, batch_size)\n",
    "    val_itr = get_data_loader(val_set, batch_size)\n",
    "    print(\"Created data loaders\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    train_loss, val_loss, train_char_acc, val_char_acc, train_ent_acc, val_ent_acc, train_sample_acc, val_sample_acc = [], [], [], [], [], [], [], []\n",
    "    epochs = []\n",
    "    iterations = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (batch_enc_text, ground_probs, __, __) in enumerate(train_itr):\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(batch_enc_text.to(torch.float32))\n",
    "            loss = criterion(pred, ground_probs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            iterations += 1\n",
    "            if iterations % 20 == 0:\n",
    "                print(\"iterations: \", iterations)\n",
    "\n",
    "        train_loss.append(get_loss(model, train_itr, criterion))\n",
    "        val_loss.append(get_loss(model, val_itr, criterion))\n",
    "\n",
    "        epochs.append(epoch)\n",
    "        tca, tea, tsa = get_accuracy(model, train_itr)\n",
    "        vca, vea, vsa = get_accuracy(model, val_itr)\n",
    "        train_char_acc.append(tca)\n",
    "        train_ent_acc.append(tea)\n",
    "        train_sample_acc.append(tsa)\n",
    "        val_char_acc.append(vca)\n",
    "        val_ent_acc.append(vea)\n",
    "        val_sample_acc.append(vsa)\n",
    "        torch.save(model.state_dict(), get_model_name(model_name, batch_size, learning_rate, epoch))\n",
    "        print(\"Epoch %d; Train Loss %f; Val Loss %f Train Char Acc %f; Val Char Acc %f Train Ent Acc %f; Val Ent Acc %f Train Sample Acc %f; Val Sample Acc %f\" % (\n",
    "            epoch+1, train_loss[-1], val_loss[-1], train_char_acc[-1], val_char_acc[-1], train_ent_acc[-1], val_ent_acc[-1], train_sample_acc[-1], val_sample_acc[-1]))\n",
    "\n",
    "    #save losses/accs\n",
    "    stats = [train_loss, val_loss, train_char_acc, val_char_acc, train_ent_acc, val_ent_acc, train_sample_acc, val_sample_acc]\n",
    "    stats_file = open(model_name + str(epoch) + '.pkl', 'wb')\n",
    "    pickle.dump(stats, stats_file)\n",
    "    stats_file.close()\n",
    "\n",
    "    # plotting\n",
    "    plt.title(\"Training Loss Curve\")\n",
    "    plt.plot(epochs, train_loss, label=\"Train\")\n",
    "    plt.plot(epochs, val_loss, label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.title(\"Training Char Acc Curve\")\n",
    "    plt.plot(epochs, train_char_acc, label=\"Train\")\n",
    "    plt.plot(epochs, val_char_acc, label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "    plt.title(\"Training Ent Acc Curve\")\n",
    "    plt.plot(epochs, train_ent_acc, label=\"Train\")\n",
    "    plt.plot(epochs, val_ent_acc, label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "    plt.title(\"Training Sample Acc Curve\")\n",
    "    plt.plot(epochs, train_sample_acc, label=\"Train\")\n",
    "    plt.plot(epochs, val_sample_acc, label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hilbert = EXT(len(char_vocab), 2*len(char_vocab))\n",
    "#train_model(hilbert, 'hilbert', train_data, val_data, 64, 20, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb Cell 20\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m thomas \u001b[39m=\u001b[39m EXT2(\u001b[39mlen\u001b[39m(char_vocab), \u001b[39m8\u001b[39m\u001b[39m*\u001b[39m\u001b[39mlen\u001b[39m(char_vocab))\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb#X25sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m train_model(thomas, \u001b[39m'\u001b[39;49m\u001b[39mthomas\u001b[39;49m\u001b[39m'\u001b[39;49m, train_data, val_data, \u001b[39m64\u001b[39;49m, \u001b[39m40\u001b[39;49m, \u001b[39m2e-4\u001b[39;49m)\n",
      "\u001b[1;32m/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb Cell 20\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_model\u001b[39m(model, model_name, train_set, val_set, batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, num_epochs\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, learning_rate\u001b[39m=\u001b[39m\u001b[39m1e-5\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb#X25sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m# DataLoaders for train and validation sets\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb#X25sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m# Create BucketIterator\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb#X25sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     train_itr \u001b[39m=\u001b[39m get_data_loader(train_set, batch_size)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb#X25sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     val_itr \u001b[39m=\u001b[39m get_data_loader(val_set, batch_size)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb#X25sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCreated data loaders\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb Cell 20\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb#X25sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     batch_text \u001b[39m=\u001b[39m [d[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m batch_data]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb#X25sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     batch_ents \u001b[39m=\u001b[39m [d[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m batch_data]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb#X25sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     batch_enc_text, ground_probs \u001b[39m=\u001b[39m get_batch_numerical(batch_data)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb#X25sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     ans\u001b[39m.\u001b[39mappend((batch_enc_text, ground_probs, batch_text, batch_ents))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb#X25sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mreturn\u001b[39;00m ans\n",
      "\u001b[1;32m/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb#X25sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m batch_ents \u001b[39m=\u001b[39m [data[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m batch_data]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb#X25sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m max_len \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(batch_text[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb#X25sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m batch_enc_text \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([text_to_onehot(text, max_len) \u001b[39mfor\u001b[39;49;00m text \u001b[39min\u001b[39;49;00m batch_text])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb#X25sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m batch_probs \u001b[39m=\u001b[39m get_batch_ground_probs(batch_text, batch_ents)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb#X25sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mreturn\u001b[39;00m batch_enc_text, batch_probs\n",
      "\u001b[1;32m/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb#X25sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m batch_ents \u001b[39m=\u001b[39m [data[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m batch_data]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb#X25sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m max_len \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(batch_text[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb#X25sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m batch_enc_text \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([text_to_onehot(text, max_len) \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m batch_text])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb#X25sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m batch_probs \u001b[39m=\u001b[39m get_batch_ground_probs(batch_text, batch_ents)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb#X25sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mreturn\u001b[39;00m batch_enc_text, batch_probs\n",
      "\u001b[1;32m/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb Cell 20\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb#X25sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m ans \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb#X25sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m char \u001b[39min\u001b[39;00m text:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb#X25sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     ans\u001b[39m.\u001b[39mappend(ident[char_to_index(char)])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb#X25sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(length \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(text)):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/admin/Documents/University/APS360/project/recpt/extractor.ipynb#X25sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     ans\u001b[39m.\u001b[39mappend(ident[char_to_index(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m)])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "thomas = EXT2(len(char_vocab), 8*len(char_vocab))\n",
    "\n",
    "train_model(thomas, 'thomas', train_data, val_data, 64, 40, 2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hannah = EXT(len(char_vocab), 4*len(char_vocab))\n",
    "hannah.load_state_dict(torch.load('model_hannah_bs64_lr0.015_epoch39'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AMANO MALAYSIA SDN BHD (682288-V) 12 JALAN PENGACARA U1/48 TEMASYA INDUSTRIAL PARK 40150 SHAH ALAM SELANGOR TEL: 03-55695002/5003 (GST ID: 001137704960) TAX INVOICE P/S #02 RM3.00 A INV-NO.0002300417000138 T/D #11 TICKET NO.029190 ENTRY TIME PAID TIME PARKING TIME 30/04/2017 (SUN) 19:44 30/04/2017 (SUN) 23:14 RM3.00 RM0.00 THANK YOU INCLUSIVE 6% GST 3:30 TYPE RATE A RM3.00 PARKING FEE GST(INCLUDED) 6.00 % RM2.83 RM0.17 TOTAL PAID CHANGE'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted entities: \n",
      "name:  AMANO MALAYSIA SDN BHD\n",
      "date:  001137704960) TAX INVOICE P/S #02 RM3.00 A INV-NO.0002300417000138 T/D #11 TICKET NO.029190 ENTRY TIME PAID TIME PARKING TIME 30/04/2017\n",
      "total:  \n",
      "actual entities: \n",
      "name:  AMANO MALAYSIA SDN BHD\n",
      "date:  30/04/2017\n",
      "total:  RM3.00\n"
     ]
    }
   ],
   "source": [
    "pred_name, pred_date, pred_address, pred_total = get_pred(hannah, test_data[0][0])\n",
    "print(\"predicted entities: \")\n",
    "print(\"name: \", pred_name)\n",
    "print(\"date: \", pred_date)\n",
    "#print(\"address: \", pred_address)\n",
    "print(\"total: \", pred_total)\n",
    "print(\"actual entities: \")\n",
    "print(\"name: \", test_data[0][1][0])\n",
    "print(\"date: \", test_data[0][1][1])\n",
    "#print(\"address: \", test_data[0][1][2])\n",
    "print(\"total: \", test_data[0][1][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = '3-1707067 F&P PHARMACY (002309592-P) NO.20, GROUNDFLOOR, SELANGOR DARUL EHSAN GST Reg NO 001880666112 JALAN BS 10/6 TAMAN BUKIT SERDANG, SEKSYEN 10, 43300 SERI KEMBANGAN, TEL 03-89599823 TAX INVOICE Doci No Cashier Salesperson 955789210525F CS00110840 F&P Date 02/03/2018 Time. 16.46.00 Ref (GST) 6.00 600 430 380 6.50 530 (GST) Amount Tax 6.00 SR 600 ZRL 430 ZRL 380 SR 650 SR 530 3190 30.68 0.00 122 0.00 31.90 50.00 18.10 Item 1486 Qty 1 1 1 1 1 S/Price S/Price 5.66 600 430 3.58 613 500 HOMECARE GASCOAL 50MG P.P NAPROXEN NA 275 MG YELLOWLOTION 30 MI. PANADOL SOLUBLE TABLET PMS GAUZE BANDAGE 5CM X 4M 9557837400035 1014 1155 95506104 DETTOL 50 ML Total Qty SR 6 Total Sales (Excluding GST) Discount Totai GST Rounding Total Sales (Inclusive of GST) : CASH : Change : GST SUMMARY Tax Code SR ZRL % 6 0 Total: : Amt (RM) 2038 1030 30.68 Tax (RM) 1.22 0.00 1.22 GOODS SOLD ARE NOT RETURNABLE & EXCHANGABLE, THANK YOU.'\n",
    "entities = [\"F&P PHARMACY\", \"02/03/2018\", \"NO.20, GROUND FLOOR, JALAN BS 10/6 TAMAN BUKIT SERDANG, SEKSYEN 10, 43300 SERI KEMBANGAN. SELANGOR DARUL EHSAN\", \"31.90\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dylan = EXT2(len(char_vocab), 4*len(char_vocab))\n",
    "dylan.load_state_dict(torch.load('models/model_dylan_bs10_lr0.0015_epoch39', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted entities: \n",
      "name:  3-1707067 F&P PHARMACY\n",
      "date:  02/03/2018\n",
      "total:  95578374\n",
      "actual entities: \n",
      "name:  F&P PHARMACY\n",
      "date:  02/03/2018\n",
      "total:  31.90\n"
     ]
    }
   ],
   "source": [
    "pred_name, pred_date, pred_address, pred_total = get_pred(dylan, s)\n",
    "print(\"predicted entities: \")\n",
    "print(\"name: \", pred_name)\n",
    "print(\"date: \", pred_date)\n",
    "#print(\"address: \", pred_address)\n",
    "print(\"total: \", pred_total)\n",
    "print(\"actual entities: \")\n",
    "print(\"name: \", entities[0])\n",
    "print(\"date: \", entities[1])\n",
    "#print(\"address: \", entities[2])\n",
    "print(\"total: \", entities[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted entities: \n",
      "name:  AMANO MALAYSIA SDN BHD\n",
      "date:  30/04/2017\n",
      "total:  RM3.00\n",
      "actual entities: \n",
      "name:  AMANO MALAYSIA SDN BHD\n",
      "date:  30/04/2017\n",
      "total:  RM3.00\n"
     ]
    }
   ],
   "source": [
    "pred_name, pred_date, pred_address, pred_total = get_pred(dylan, test_data[0][0])\n",
    "print(\"predicted entities: \")\n",
    "print(\"name: \", pred_name)\n",
    "print(\"date: \", pred_date)\n",
    "#print(\"address: \", pred_address)\n",
    "print(\"total: \", pred_total)\n",
    "print(\"actual entities: \")\n",
    "print(\"name: \", test_data[0][1][0])\n",
    "print(\"date: \", test_data[0][1][1])\n",
    "#print(\"address: \", test_data[0][1][2])\n",
    "print(\"total: \", test_data[0][1][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8307773194607501, 0.797945205479452, 0.410958904109589)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_itr = get_data_loader(test_data, 32)\n",
    "get_accuracy(dylan, test_itr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
