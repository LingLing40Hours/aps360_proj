{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Layer):\n",
    "    def __init__(self, in_channels, out_channels, hidden_size, **kwargs):\n",
    "        super(AttentionHead, self).__init__()\n",
    "        self.input_size = in_channels\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_classes = out_channels\n",
    "\n",
    "        self.attention_cell = AttentionGRUCell(\n",
    "            in_channels, hidden_size, out_channels, use_gru=False)\n",
    "        self.generator = nn.Linear(hidden_size, out_channels)\n",
    "\n",
    "    def _char_to_onehot(self, input_char, onehot_dim):\n",
    "        input_ont_hot = F.one_hot(input_char, onehot_dim)\n",
    "        return input_ont_hot\n",
    "\n",
    "    def forward(self, inputs, data=None, batch_max_length=25):\n",
    "        batch_size = paddle.shape(inputs)[0]\n",
    "        num_steps = batch_max_length\n",
    "\n",
    "        hidden = paddle.zeros((batch_size, self.hidden_size))\n",
    "        output_hiddens = []\n",
    "\n",
    "        if data is not None:\n",
    "            targets = data[1]\n",
    "            for i in range(num_steps):\n",
    "                char_onehots = self._char_to_onehot(\n",
    "                    targets[:, i], onehot_dim=self.num_classes)\n",
    "                (outputs, hidden), alpha = self.attention_cell(hidden, inputs,\n",
    "                                                               char_onehots)\n",
    "                output_hiddens.append(paddle.unsqueeze(outputs, axis=1))\n",
    "            output = paddle.concat(output_hiddens, axis=1)\n",
    "            probs = self.generator(output)\n",
    "        else:\n",
    "            targets = paddle.zeros(shape=[batch_size], dtype=\"int32\")\n",
    "            probs = None\n",
    "            char_onehots = None\n",
    "            outputs = None\n",
    "            alpha = None\n",
    "\n",
    "            for i in range(num_steps):\n",
    "                char_onehots = self._char_to_onehot(\n",
    "                    targets, onehot_dim=self.num_classes)\n",
    "                (outputs, hidden), alpha = self.attention_cell(hidden, inputs,\n",
    "                                                               char_onehots)\n",
    "                probs_step = self.generator(outputs)\n",
    "                if probs is None:\n",
    "                    probs = paddle.unsqueeze(probs_step, axis=1)\n",
    "                else:\n",
    "                    probs = paddle.concat(\n",
    "                        [probs, paddle.unsqueeze(\n",
    "                            probs_step, axis=1)], axis=1)\n",
    "                next_input = probs_step.argmax(axis=1)\n",
    "                targets = next_input\n",
    "        if not self.training:\n",
    "            probs = paddle.nn.functional.softmax(probs, axis=2)\n",
    "        return {'res':probs}\n",
    "\n",
    "\n",
    "class AttentionLSTM(nn.Layer):\n",
    "    def __init__(self, in_channels, out_channels, hidden_size, **kwargs):\n",
    "        super(AttentionLSTM, self).__init__()\n",
    "        self.input_size = in_channels\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_classes = out_channels\n",
    "\n",
    "        self.attention_cell = AttentionLSTMCell(\n",
    "            in_channels, hidden_size, out_channels, use_gru=False)\n",
    "        self.generator = nn.Linear(hidden_size, out_channels)\n",
    "\n",
    "    def _char_to_onehot(self, input_char, onehot_dim):\n",
    "        input_ont_hot = F.one_hot(input_char, onehot_dim)\n",
    "        return input_ont_hot\n",
    "\n",
    "    def forward(self, inputs, targets=None, batch_max_length=25):\n",
    "        batch_size = inputs.shape[0]\n",
    "        num_steps = batch_max_length\n",
    "\n",
    "        hidden = (paddle.zeros((batch_size, self.hidden_size)), paddle.zeros(\n",
    "            (batch_size, self.hidden_size)))\n",
    "        output_hiddens = []\n",
    "\n",
    "        if targets is not None:\n",
    "            for i in range(num_steps):\n",
    "                # one-hot vectors for a i-th char\n",
    "                char_onehots = self._char_to_onehot(\n",
    "                    targets[:, i], onehot_dim=self.num_classes)\n",
    "                hidden, alpha = self.attention_cell(hidden, inputs,\n",
    "                                                    char_onehots)\n",
    "\n",
    "                hidden = (hidden[1][0], hidden[1][1])\n",
    "                output_hiddens.append(paddle.unsqueeze(hidden[0], axis=1))\n",
    "            output = paddle.concat(output_hiddens, axis=1)\n",
    "            probs = self.generator(output)\n",
    "\n",
    "        else:\n",
    "            targets = paddle.zeros(shape=[batch_size], dtype=\"int32\")\n",
    "            probs = None\n",
    "            char_onehots = None\n",
    "            alpha = None\n",
    "\n",
    "            for i in range(num_steps):\n",
    "                char_onehots = self._char_to_onehot(\n",
    "                    targets, onehot_dim=self.num_classes)\n",
    "                hidden, alpha = self.attention_cell(hidden, inputs,\n",
    "                                                    char_onehots)\n",
    "                probs_step = self.generator(hidden[0])\n",
    "                hidden = (hidden[1][0], hidden[1][1])\n",
    "                if probs is None:\n",
    "                    probs = paddle.unsqueeze(probs_step, axis=1)\n",
    "                else:\n",
    "                    probs = paddle.concat(\n",
    "                        [probs, paddle.unsqueeze(\n",
    "                            probs_step, axis=1)], axis=1)\n",
    "\n",
    "                next_input = probs_step.argmax(axis=1)\n",
    "\n",
    "                targets = next_input\n",
    "        if not self.training:\n",
    "            probs = paddle.nn.functional.softmax(probs, axis=2)\n",
    "        return probs\n",
    "\n",
    "\n",
    "class AttentionLSTMCell(nn.Layer):\n",
    "    def __init__(self, input_size, hidden_size, num_embeddings, use_gru=False):\n",
    "        super(AttentionLSTMCell, self).__init__()\n",
    "        self.i2h = nn.Linear(input_size, hidden_size, bias_attr=False)\n",
    "        self.h2h = nn.Linear(hidden_size, hidden_size)\n",
    "        self.score = nn.Linear(hidden_size, 1, bias_attr=False)\n",
    "        self.rnn = nn.LSTMCell(\n",
    "            input_size=input_size + num_embeddings, hidden_size=hidden_size)\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, prev_hidden, batch_H, char_onehots):\n",
    "        batch_H_proj = self.i2h(batch_H)\n",
    "        prev_hidden_proj = paddle.unsqueeze(self.h2h(prev_hidden[0]), axis=1)\n",
    "        res = paddle.add(batch_H_proj, prev_hidden_proj)\n",
    "        res = paddle.tanh(res)\n",
    "        e = self.score(res)\n",
    "\n",
    "        alpha = F.softmax(e, axis=1)\n",
    "        alpha = paddle.transpose(alpha, [0, 2, 1])\n",
    "        context = paddle.squeeze(paddle.mm(alpha, batch_H), axis=1)\n",
    "        concat_context = paddle.concat([context, char_onehots], 1)\n",
    "        cur_hidden = self.rnn(concat_context, prev_hidden)\n",
    "\n",
    "        return cur_hidden, alpha"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
