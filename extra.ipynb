{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCR_F(nn.Module):\n",
    "    def __init__(self, charset_size, hidden_size, lstm_layers):\n",
    "        assert(charset_size < 256)\n",
    "        super(OCR_F, self).__init__()\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(6, 15, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(15, 42, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.lstm = nn.LSTM(42 * 103 * 212, hidden_size, lstm_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size, charset_size)\n",
    "\n",
    "    def forward(self, x): #[1, 825, 1697]\n",
    "        x = self.pool1(F.relu(self.conv1(x))) #[6, 412, 848]\n",
    "        x = self.pool2(F.relu(self.conv2(x))) #[15, 206, 424]\n",
    "        x = self.pool3(F.relu(self.conv3(x))) #[42, 103, 212]\n",
    "        x = x.view(x.size(0), -1)\n",
    "        h0 = torch.zeros(self.lstm_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.lstm_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, __ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCR_S(nn.Module):\n",
    "    def __init__(self, charset_size, hidden_size, lstm_layers):\n",
    "        super(OCR_S, self).__init__()\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(6, 15, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(15, 42, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # For sequence-to-sequence, we'll use an LSTM cell as the decoder\n",
    "        self.decoder_lstm = nn.LSTMCell(charset_size, hidden_size * 2)\n",
    "        self.character_prob = nn.Linear(hidden_size * 2, charset_size)\n",
    "\n",
    "    def forward(self, x, max_output_length):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = self.pool3(F.relu(self.conv3(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #x = x.view(x.size(0), 1, -1)  # Reshape for LSTM input\n",
    "        \n",
    "        h0 = torch.zeros(x.size(0), self.hidden_size * 2).to(x.device)\n",
    "        c0 = torch.zeros(x.size(0), self.hidden_size * 2).to(x.device)\n",
    "\n",
    "        hidden_states = [h0]\n",
    "        cell_states = [c0]\n",
    "        predictions = []\n",
    "\n",
    "        for _ in range(max_output_length):\n",
    "            input = hidden_states[-1]\n",
    "            h, c = self.decoder_lstm(input, (hidden_states[-1], cell_states[-1]))\n",
    "            hidden_states.append(h)\n",
    "            cell_states.append(c)\n",
    "            pred = self.character_prob(h)\n",
    "            predictions.append(pred)\n",
    "\n",
    "        return torch.stack(predictions, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.linear_q = nn.Linear(input_size, hidden_size)\n",
    "        self.linear_k = nn.Linear(input_size, hidden_size)\n",
    "        self.linear_v = nn.Linear(input_size, hidden_size)\n",
    "        self.linear_x = nn.Linear(input_size, hidden_size)\n",
    "        self.attention = nn.MultiheadAttention(hidden_size, num_heads=4, batch_first=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size)\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        q, k, v = self.linear_q(x), self.linear_k(x), self.linear_v(x)\n",
    "        x = self.norm(self.linear_x(x) + self.attention(q, k, v))\n",
    "        x = self.norm(x + self.fc(x))\n",
    "        return x\n",
    "\n",
    "class MultiLayerTransformer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(MultiLayerTransformer, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoder(input_size, hidden_size) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Texts: ('String 1', 'String 2', 'String 3')\n",
      "Batch Entities: (['A', 'B', 'C', 'D'], ['E', 'F', 'G', 'H'], ['D', 'F', 'G', 'H'])\n",
      "Batch Texts: ('String 4', 'String 5', 'String 6')\n",
      "Batch Entities: (['F', 'F', 'G', 'H'], ['G', 'F', 'G', 'H'], ['H', 'F', 'G', 'H'])\n",
      "Batch Texts: ('String 7', 'String 8')\n",
      "Batch Entities: (['I', 'F', 'G', 'H'], ['J', 'F', 'G', 'H'])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text, entities = self.data[index]\n",
    "        return text, entities\n",
    "\n",
    "def custom_collate(batch):\n",
    "    texts, entities = zip(*batch)\n",
    "    return texts, entities\n",
    "\n",
    "# Your example data\n",
    "example_list = [\n",
    "    [\"String 1\", [\"A\", \"B\", \"C\", \"D\"]],\n",
    "    [\"String 2\", [\"E\", \"F\", \"G\", \"H\"]],\n",
    "    [\"String 3\", [\"D\", \"F\", \"G\", \"H\"]],\n",
    "    [\"String 4\", [\"F\", \"F\", \"G\", \"H\"]],\n",
    "    [\"String 5\", [\"G\", \"F\", \"G\", \"H\"]],\n",
    "    [\"String 6\", [\"H\", \"F\", \"G\", \"H\"]],\n",
    "    [\"String 7\", [\"I\", \"F\", \"G\", \"H\"]],\n",
    "    [\"String 8\", [\"J\", \"F\", \"G\", \"H\"]],\n",
    "    # Add more entries as needed\n",
    "]\n",
    "\n",
    "custom_dataset = CustomDataset(example_list)\n",
    "\n",
    "custom_dataloader = DataLoader(\n",
    "    custom_dataset, batch_size=3, collate_fn=custom_collate\n",
    ")\n",
    "\n",
    "for batch_texts, batch_entities in custom_dataloader:\n",
    "    print(\"Batch Texts:\", batch_texts)\n",
    "    print(\"Batch Entities:\", batch_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExSet(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, text, entities = self.data[index]\n",
    "        return text, entities\n",
    "\n",
    "def custom_collate(batch):\n",
    "    texts, entities = zip(*batch)\n",
    "    return texts, entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = ExSet(train_data)\n",
    "val_set = ExSet(val_data)\n",
    "test_set = ExSet(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "train_data, temp_data = train_test_split(records, test_size=(1 - train_ratio))\n",
    "val_data, test_data = train_test_split(temp_data, test_size=test_ratio / (val_ratio + test_ratio))\n",
    "print(\"Training set len:\", len(train_data))\n",
    "print(\"Validation set len:\", len(val_data))\n",
    "print(\"Test set len:\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'records' in globals():\n",
    "    del records\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_vocab = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',\n",
    "              'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
    "              '`', '1', '2', '3', '4', '5', '6', '7', '8', '9', '0', '-', '=', '~', '!', '@', '#', '$', '%', '^', '&', '*', '(', ')', '_', '+',\n",
    "              '[', ']', '\\\\', ';', '\\'', ',', '.', '?', '{', '}', '|', ':', '\\\"', '<', '>', '?']\n",
    "\n",
    "def index_to_char(i):\n",
    "    return char_vocab[i]\n",
    "\n",
    "def char_to_index(c):\n",
    "    return char_vocab.index(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchtext.legacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/admin/Documents/University/APS360/project/recpt/extra.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/admin/Documents/University/APS360/project/recpt/extra.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchtext\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlegacy\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchtext.legacy'"
     ]
    }
   ],
   "source": [
    "import torchtext.legacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchtext.legacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/admin/Documents/University/APS360/project/recpt/extra.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/admin/Documents/University/APS360/project/recpt/extra.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchtext\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlegacy\u001b[39;00m \u001b[39mimport\u001b[39;00m data\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchtext.legacy'"
     ]
    }
   ],
   "source": [
    "from torchtext.legacy import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True, collate_fn=custom_collate)\n",
    "for texts, ents in train_loader:\n",
    "    print(len(texts), len(ents))\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
