{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCR_F(nn.Module):\n",
    "    def __init__(self, charset_size, hidden_size, lstm_layers):\n",
    "        assert(charset_size < 256)\n",
    "        super(OCR_F, self).__init__()\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(6, 15, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(15, 42, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.lstm = nn.LSTM(42 * 103 * 212, hidden_size, lstm_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size, charset_size)\n",
    "\n",
    "    def forward(self, x): #[1, 825, 1697]\n",
    "        x = self.pool1(F.relu(self.conv1(x))) #[6, 412, 848]\n",
    "        x = self.pool2(F.relu(self.conv2(x))) #[15, 206, 424]\n",
    "        x = self.pool3(F.relu(self.conv3(x))) #[42, 103, 212]\n",
    "        x = x.view(x.size(0), -1)\n",
    "        h0 = torch.zeros(self.lstm_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.lstm_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, __ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCR_S(nn.Module):\n",
    "    def __init__(self, charset_size, hidden_size, lstm_layers):\n",
    "        super(OCR_S, self).__init__()\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(6, 15, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(15, 42, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # For sequence-to-sequence, we'll use an LSTM cell as the decoder\n",
    "        self.decoder_lstm = nn.LSTMCell(charset_size, hidden_size * 2)\n",
    "        self.character_prob = nn.Linear(hidden_size * 2, charset_size)\n",
    "\n",
    "    def forward(self, x, max_output_length):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = self.pool3(F.relu(self.conv3(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #x = x.view(x.size(0), 1, -1)  # Reshape for LSTM input\n",
    "        \n",
    "        h0 = torch.zeros(x.size(0), self.hidden_size * 2).to(x.device)\n",
    "        c0 = torch.zeros(x.size(0), self.hidden_size * 2).to(x.device)\n",
    "\n",
    "        hidden_states = [h0]\n",
    "        cell_states = [c0]\n",
    "        predictions = []\n",
    "\n",
    "        for _ in range(max_output_length):\n",
    "            input = hidden_states[-1]\n",
    "            h, c = self.decoder_lstm(input, (hidden_states[-1], cell_states[-1]))\n",
    "            hidden_states.append(h)\n",
    "            cell_states.append(c)\n",
    "            pred = self.character_prob(h)\n",
    "            predictions.append(pred)\n",
    "\n",
    "        return torch.stack(predictions, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.linear_q = nn.Linear(input_size, hidden_size)\n",
    "        self.linear_k = nn.Linear(input_size, hidden_size)\n",
    "        self.linear_v = nn.Linear(input_size, hidden_size)\n",
    "        self.linear_x = nn.Linear(input_size, hidden_size)\n",
    "        self.attention = nn.MultiheadAttention(hidden_size, num_heads=4, batch_first=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size)\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        q, k, v = self.linear_q(x), self.linear_k(x), self.linear_v(x)\n",
    "        x = self.norm(self.linear_x(x) + self.attention(q, k, v))\n",
    "        x = self.norm(x + self.fc(x))\n",
    "        return x\n",
    "\n",
    "class MultiLayerTransformer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(MultiLayerTransformer, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoder(input_size, hidden_size) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Texts: ('String 1', 'String 2', 'String 3')\n",
      "Batch Entities: (['A', 'B', 'C', 'D'], ['E', 'F', 'G', 'H'], ['D', 'F', 'G', 'H'])\n",
      "Batch Texts: ('String 4', 'String 5', 'String 6')\n",
      "Batch Entities: (['F', 'F', 'G', 'H'], ['G', 'F', 'G', 'H'], ['H', 'F', 'G', 'H'])\n",
      "Batch Texts: ('String 7', 'String 8')\n",
      "Batch Entities: (['I', 'F', 'G', 'H'], ['J', 'F', 'G', 'H'])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text, entities = self.data[index]\n",
    "        return text, entities\n",
    "\n",
    "def custom_collate(batch):\n",
    "    texts, entities = zip(*batch)\n",
    "    return texts, entities\n",
    "\n",
    "# Your example data\n",
    "example_list = [\n",
    "    [\"String 1\", [\"A\", \"B\", \"C\", \"D\"]],\n",
    "    [\"String 2\", [\"E\", \"F\", \"G\", \"H\"]],\n",
    "    [\"String 3\", [\"D\", \"F\", \"G\", \"H\"]],\n",
    "    [\"String 4\", [\"F\", \"F\", \"G\", \"H\"]],\n",
    "    [\"String 5\", [\"G\", \"F\", \"G\", \"H\"]],\n",
    "    [\"String 6\", [\"H\", \"F\", \"G\", \"H\"]],\n",
    "    [\"String 7\", [\"I\", \"F\", \"G\", \"H\"]],\n",
    "    [\"String 8\", [\"J\", \"F\", \"G\", \"H\"]],\n",
    "    # Add more entries as needed\n",
    "]\n",
    "\n",
    "custom_dataset = CustomDataset(example_list)\n",
    "\n",
    "custom_dataloader = DataLoader(\n",
    "    custom_dataset, batch_size=3, collate_fn=custom_collate\n",
    ")\n",
    "\n",
    "for batch_texts, batch_entities in custom_dataloader:\n",
    "    print(\"Batch Texts:\", batch_texts)\n",
    "    print(\"Batch Entities:\", batch_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExSet(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, text, entities = self.data[index]\n",
    "        return text, entities\n",
    "\n",
    "def custom_collate(batch):\n",
    "    texts, entities = zip(*batch)\n",
    "    return texts, entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = ExSet(train_data)\n",
    "val_set = ExSet(val_data)\n",
    "test_set = ExSet(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "train_data, temp_data = train_test_split(records, test_size=(1 - train_ratio))\n",
    "val_data, test_data = train_test_split(temp_data, test_size=test_ratio / (val_ratio + test_ratio))\n",
    "print(\"Training set len:\", len(train_data))\n",
    "print(\"Validation set len:\", len(val_data))\n",
    "print(\"Test set len:\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'records' in globals():\n",
    "    del records\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The substring 'sub string' (ignoring spaces) first occurs at index: 37\n"
     ]
    }
   ],
   "source": [
    "# Sample string\n",
    "main_string = \"This is a sample string containing a substring.\"\n",
    "\n",
    "# Substring to find (with spaces)\n",
    "substring = \"sub string\"\n",
    "\n",
    "# Remove spaces from both main string and substring for comparison\n",
    "main_string_no_spaces = main_string.replace(\" \", \"\")\n",
    "substring_no_spaces = substring.replace(\" \", \"\")\n",
    "\n",
    "# Get the index where the substring (ignoring spaces) occurs in the modified main string\n",
    "index = main_string_no_spaces.find(substring_no_spaces)\n",
    "\n",
    "if index != -1:\n",
    "    # Calculate the adjusted index considering spaces in the original string\n",
    "    adjusted_index = 0\n",
    "    spaces_count = 0\n",
    "    for i in range(len(main_string)):\n",
    "        if main_string[i] != \" \":\n",
    "            adjusted_index += 1\n",
    "        else:\n",
    "            spaces_count += 1\n",
    "        if adjusted_index - spaces_count == index:\n",
    "            break\n",
    "\n",
    "    print(f\"The substring '{substring}' (ignoring spaces) first occurs at index: {adjusted_index}\")\n",
    "else:\n",
    "    print(f\"The substring '{substring}' (ignoring spaces) does not exist in the main string.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'substring.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_string[37:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The substring 'sub string' (ignoring spaces) starts at index: 30\n",
      "The substring 'sub string' (ignoring spaces) ends at index: 40\n"
     ]
    }
   ],
   "source": [
    "# Sample string\n",
    "main_string = \"This is a sample string containing a substring.\"\n",
    "\n",
    "# Substring to find (with spaces)\n",
    "substring = \"sub string\"\n",
    "\n",
    "# Remove spaces from both main string and substring for comparison\n",
    "main_string_no_spaces = main_string.replace(\" \", \"\")\n",
    "substring_no_spaces = substring.replace(\" \", \"\")\n",
    "\n",
    "# Get the index where the substring (ignoring spaces) occurs in the modified main string\n",
    "start_index = main_string_no_spaces.find(substring_no_spaces)\n",
    "\n",
    "if start_index != -1:\n",
    "    end_index = start_index + len(substring_no_spaces)\n",
    "\n",
    "    # Calculate the adjusted indexes considering spaces in the original string\n",
    "    adjusted_end_index = 0\n",
    "    spaces_count = 0\n",
    "\n",
    "    for i in range(len(main_string)):\n",
    "        if main_string[i] != \" \":\n",
    "            adjusted_end_index += 1\n",
    "        else:\n",
    "            spaces_count += 1\n",
    "        if adjusted_end_index - spaces_count == end_index:\n",
    "            break\n",
    "\n",
    "    print(f\"The substring '{substring}' (ignoring spaces) starts at index: {start_index}\")\n",
    "    print(f\"The substring '{substring}' (ignoring spaces) ends at index: {adjusted_end_index}\")\n",
    "else:\n",
    "    print(f\"The substring '{substring}' (ignoring spaces) does not exist in the main string.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(main_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchtext.legacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/admin/Documents/University/APS360/project/recpt/extra.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/admin/Documents/University/APS360/project/recpt/extra.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchtext\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlegacy\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchtext.legacy'"
     ]
    }
   ],
   "source": [
    "import torchtext.legacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchtext.legacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/admin/Documents/University/APS360/project/recpt/extra.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/admin/Documents/University/APS360/project/recpt/extra.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchtext\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlegacy\u001b[39;00m \u001b[39mimport\u001b[39;00m data\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchtext.legacy'"
     ]
    }
   ],
   "source": [
    "from torchtext.legacy import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True, collate_fn=custom_collate)\n",
    "for texts, ents in train_loader:\n",
    "    print(len(texts), len(ents))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "test = torch.rand(3, 4, 2)\n",
    "test = F.softmax(test, dim=1)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2850, 0.2242],\n",
       "         [0.1947, 0.1796],\n",
       "         [0.2689, 0.4012],\n",
       "         [0.2514, 0.1949]],\n",
       "\n",
       "        [[0.1820, 0.2570],\n",
       "         [0.3742, 0.1824],\n",
       "         [0.1954, 0.3900],\n",
       "         [0.2483, 0.1707]],\n",
       "\n",
       "        [[0.2207, 0.2231],\n",
       "         [0.2863, 0.3196],\n",
       "         [0.2681, 0.1791],\n",
       "         [0.2250, 0.2782]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.argmax(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 1, 0],\n",
       "        [1, 0, 1, 0],\n",
       "        [1, 1, 0, 1]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_field = Field(sequential=True, tokenize=lambda x: x, include_lengths=True, batch_first=True, use_vocab=True)\n",
    "name_field = Field(sequential=False)\n",
    "date_field = Field(sequential=False)\n",
    "address_field = Field(sequential=False)\n",
    "total_field = Field(sequential=False)\n",
    "\n",
    "# Create examples from dataset\n",
    "data_examples = [Example.fromlist([text, name, date, address, total], fields=[\n",
    "    ('text', text_field), ('name', name_field), ('date', date_field), ('address', address_field), ('total', total_field)\n",
    "    ]) for text, (name, date, address, total) in records]\n",
    "data_set = Dataset(data_examples, fields=[('text', text_field), ('name', name_field), ('date', date_field), ('address', address_field), ('total', total_field)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocab (if necessary)\n",
    "text_field.build_vocab(data_set)\n",
    "name_field.build_vocab(data_set)\n",
    "date_field.build_vocab(data_set)\n",
    "address_field.build_vocab(data_set)\n",
    "\n",
    "\n",
    "vocab_stoi = text_field.vocab.stoi # so we don't have to rewrite sample_sequence\n",
    "vocab_itos = text_field.vocab.itos # so we don't have to rewrite sample_sequence\n",
    "vocab_size = len(text_field.vocab.itos)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def character_error_rate(reference, hypothesis):\n",
    "    \"\"\"\n",
    "    Computes the Character Error Rate (CER) between two strings.\n",
    "\n",
    "    Args:\n",
    "    reference (str): The reference string.\n",
    "    hypothesis (str): The hypothesis string.\n",
    "\n",
    "    Returns:\n",
    "    float: Character Error Rate between the two strings (between 0 and 1).\n",
    "    \"\"\"\n",
    "    # Ensure strings are not empty\n",
    "    if len(reference) == 0 and len(hypothesis) == 0:\n",
    "        return 0.0\n",
    "    elif len(reference) == 0 or len(hypothesis) == 0:\n",
    "        return 1.0\n",
    "    \n",
    "    # Initialize variables for CER calculation\n",
    "    n = len(reference)\n",
    "    m = len(hypothesis)\n",
    "    dp = [[0] * (m + 1) for _ in range(n + 1)]\n",
    "\n",
    "    # Initialize DP matrix\n",
    "    for i in range(n + 1):\n",
    "        dp[i][0] = i\n",
    "\n",
    "    for j in range(m + 1):\n",
    "        dp[0][j] = j\n",
    "\n",
    "    # Fill DP matrix\n",
    "    for i in range(1, n + 1):\n",
    "        for j in range(1, m + 1):\n",
    "            cost = 0 if reference[i - 1] == hypothesis[j - 1] else 1\n",
    "            dp[i][j] = min(dp[i - 1][j] + 1, dp[i][j - 1] + 1, dp[i - 1][j - 1] + cost)\n",
    "\n",
    "    return dp[n][m] / n  # Normalized CER between 0 and 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "character_error_rate('sjlskclx', 'skdhjsoiucx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cer(reference, hypothesis):\n",
    "    # Convert the sentences into character lists\n",
    "    ref = list(reference)\n",
    "    hyp = list(hypothesis)\n",
    "\n",
    "    # Create a matrix of size (len(ref)+1) x (len(hyp)+1)\n",
    "    d = np.zeros((len(ref) + 1) * (len(hyp) + 1), dtype=np.uint32)\n",
    "    d = d.reshape((len(ref) + 1, len(hyp) + 1))\n",
    "\n",
    "    # Initialize the first row and column to be the distance from the empty string\n",
    "    for i in range(len(ref) + 1):\n",
    "        d[i][0] = i\n",
    "    for j in range(len(hyp) + 1):\n",
    "        d[0][j] = j\n",
    "\n",
    "    # Populate the rest of the matrix\n",
    "    for i in range(1, len(ref) + 1):\n",
    "        for j in range(1, len(hyp) + 1):\n",
    "            if ref[i - 1] == hyp[j - 1]:\n",
    "                cost = 0\n",
    "            else:\n",
    "                cost = 1\n",
    "            d[i][j] = min(d[i - 1][j] + 1,      # deletion\n",
    "                          d[i][j - 1] + 1,      # insertion\n",
    "                          d[i - 1][j - 1] + cost)  # substitution\n",
    "\n",
    "    # The CER is the cost of transforming hypothesis into reference divided by the number of characters in the reference\n",
    "    cer_value = float(d[len(ref)][len(hyp)]) / len(ref)\n",
    "\n",
    "    return cer_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0666666666666667"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "cer(\"i alsiwoclzjvcx\", \"dkss_dsl iohk lsks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
