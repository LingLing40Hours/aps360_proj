{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":4366,"status":"ok","timestamp":1701215898383,"user":{"displayName":"Georgia Chen","userId":"17843376469141931868"},"user_tz":300},"id":"mS-3XaWvXqqR"},"outputs":[],"source":["import os\n","import time\n","import random\n","import shutil\n","import torch\n","import pandas as pd\n","import numpy as np\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchvision\n","import matplotlib.pyplot as plt\n","from torchvision import datasets, models, transforms\n","from torch.utils.data import TensorDataset\n","from torch.utils.data.sampler import SubsetRandomSampler\n","from torchvision.datasets import ImageFolder\n","from torch.utils.data import DataLoader\n","import matplotlib.pyplot as plt\n","from PIL import Image, ImageOps"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5861,"status":"ok","timestamp":1701215904238,"user":{"displayName":"Georgia Chen","userId":"17843376469141931868"},"user_tz":300},"id":"RAeJFkVVm-Y1","outputId":"6dd37aa5-7fc2-4d40-f7e1-75a226969baf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting python-Levenshtein\n","  Obtaining dependency information for python-Levenshtein from https://files.pythonhosted.org/packages/27/89/c45dbdbd479453cfc8c4c1271c9f67fd607deaf84880e31c67b682980456/python_Levenshtein-0.23.0-py3-none-any.whl.metadata\n","  Downloading python_Levenshtein-0.23.0-py3-none-any.whl.metadata (3.8 kB)\n","Collecting Levenshtein==0.23.0 (from python-Levenshtein)\n","  Obtaining dependency information for Levenshtein==0.23.0 from https://files.pythonhosted.org/packages/12/06/c4a6bbc804d027ccc2b0229adb7ed733b63db34bb18d261aeae6c2fb2ea4/Levenshtein-0.23.0-cp311-cp311-macosx_10_9_x86_64.whl.metadata\n","  Downloading Levenshtein-0.23.0-cp311-cp311-macosx_10_9_x86_64.whl.metadata (3.4 kB)\n","Collecting rapidfuzz<4.0.0,>=3.1.0 (from Levenshtein==0.23.0->python-Levenshtein)\n","  Obtaining dependency information for rapidfuzz<4.0.0,>=3.1.0 from https://files.pythonhosted.org/packages/5d/47/309addcb9e8d818a8456956ffa9f4db57be11187515b64d24ca0d500e3a4/rapidfuzz-3.5.2-cp311-cp311-macosx_10_9_x86_64.whl.metadata\n","  Downloading rapidfuzz-3.5.2-cp311-cp311-macosx_10_9_x86_64.whl.metadata (11 kB)\n","Downloading python_Levenshtein-0.23.0-py3-none-any.whl (9.4 kB)\n","Downloading Levenshtein-0.23.0-cp311-cp311-macosx_10_9_x86_64.whl (132 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading rapidfuzz-3.5.2-cp311-cp311-macosx_10_9_x86_64.whl (2.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein, python-Levenshtein\n","Successfully installed Levenshtein-0.23.0 python-Levenshtein-0.23.0 rapidfuzz-3.5.2\n","\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"]}],"source":["!pip3 install python-Levenshtein"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":724,"status":"ok","timestamp":1701215904955,"user":{"displayName":"Georgia Chen","userId":"17843376469141931868"},"user_tz":300},"id":"LZVANB0ZnQyj","outputId":"288da7bf-f5d8-46f4-97fa-b5a02cfb5954"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1701215904956,"user":{"displayName":"Georgia Chen","userId":"17843376469141931868"},"user_tz":300},"id":"CqeTUpgEm_AH"},"outputs":[],"source":["from Levenshtein import distance as levenshtein_distance\n","\n","def wer(reference, hypothesis):\n","    # Split the sentences into words\n","    ref = reference.split()\n","    hyp = hypothesis.split()\n","\n","    # Create a matrix of size (len(ref)+1) x (len(hyp)+1)\n","    d = np.zeros((len(ref) + 1) * (len(hyp) + 1), dtype=np.uint8)\n","    d = d.reshape((len(ref) + 1, len(hyp) + 1))\n","\n","    # Initialize the first row and column\n","    for i in range(len(ref) + 1):\n","        d[i][0] = i\n","    for j in range(len(hyp) + 1):\n","        d[0][j] = j\n","\n","    # Populate the matrix\n","    for i in range(1, len(ref) + 1):\n","        for j in range(1, len(hyp) + 1):\n","            if ref[i - 1] == hyp[j - 1]:\n","                cost = 0\n","            else:\n","                cost = 1\n","            d[i][j] = min(d[i - 1][j] + 1,      # deletion\n","                          d[i][j - 1] + 1,      # insertion\n","                          d[i - 1][j - 1] + cost)  # substitution\n","\n","    # Compute WER\n","    wer_value = float(d[len(ref)][len(hyp)]) / len(ref)\n","\n","    return wer_value"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1701215904956,"user":{"displayName":"Georgia Chen","userId":"17843376469141931868"},"user_tz":300},"id":"LoWuTfGAVZRL"},"outputs":[],"source":["def ground_truth(file_name):\n","  #ground truth = [date,name,total]\n","  f = open(file_name, 'r')\n","  content = f.read()\n","\n","  #print(content)\n","  content_split = content.split(\"\\n\")\n","\n","  #print(content_split)\n","  ground_label = [0,0,0]\n","\n","  for i in content_split:\n","    if \"date\" in i:\n","      a = i.split(\":\")\n","      a[1] = a[1].replace(\",\",\"\")\n","      a[1] = a[1].replace('\"',\"\")\n","      #print(\"date \", a[1])\n","      ground_label[0] = a[1]\n","\n","    elif \"company\" in i:\n","      a = i.split(\":\")\n","      a[1] = a[1].replace(\",\",\"\")\n","      a[1] = a[1].replace('\"',\"\")\n","      #print(\"name \", a[1])\n","      ground_label[1] = a[1]\n","\n","    elif \"total\" in i:\n","      a = i.split(\":\")\n","      a[1] = a[1].replace(\",\",\"\")\n","      a[1] = a[1].replace('\"',\"\")\n","      #print(\"total \", a[1])\n","      ground_label[2] = a[1]\n","\n","  return(ground_label)"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1701215905103,"user":{"displayName":"Georgia Chen","userId":"17843376469141931868"},"user_tz":300},"id":"1CBXHEAtY5vW","outputId":"c0cd9bd5-61f4-453e-b475-3fabce174858"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of classes: 91\n"]}],"source":["# Example dataset containing unique characters\n","unique_characters = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789!@#$%^&*()_+-=[]{};:,.<>?/\\\\| \"\n","\n","# Calculate the number of unique classes\n","NUM_CLASSES = len(unique_characters)\n","print(\"Number of classes:\", NUM_CLASSES)"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1701215905103,"user":{"displayName":"Georgia Chen","userId":"17843376469141931868"},"user_tz":300},"id":"F51p-VIUXy4A"},"outputs":[],"source":["num_classes = NUM_CLASSES\n","class OCRModel(nn.Module):\n","    def __init__(self, num_classes):\n","        super(OCRModel, self).__init__()\n","        # CNN layers\n","        self.cnn = nn.Sequential(\n","            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),  # Output: 32x128 -> 32x128 (after pooling: 16x64)\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2),  # Reduces dimensions by half\n","            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # Output: 16x64 -> 16x64 (after pooling: 8x32)\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2),  # Further reduces dimensions by half\n","            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1), # Additional layer, Output: 8x32 -> 8x32 (after pooling: 4x16)\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2),\n","        )\n","        # RNN layers\n","        # Input size calculation: Final CNN output size is [batch_size, 128 channels, 4 height, 16 width]\n","        # Flattening height into channels -> [batch_size, 512, 16]\n","        self.rnn = nn.LSTM(input_size=512, hidden_size=256, num_layers=2, batch_first=True, bidirectional=True)\n","        # Fully connected layer\n","        self.fc = nn.Linear(512, num_classes)\n","\n","    def forward(self, x):\n","        # CNN\n","        x = self.cnn(x)\n","        print(\"Shape after CNN:\", x.shape)  # Check shape after CNN layers\n","\n","        # Flattening the height and channel dimensions\n","        batch_size, channels, height, width = x.size()\n","        x = x.view(batch_size, channels * height, width)  # New shape: [batch_size, channels * height, width]\n","        print(\"Shape after flattening:\", x.shape)\n","\n","        x = x.permute(0, 2, 1)  # Permute to make width as sequence length\n","        # RNN\n","        x, _ = self.rnn(x)\n","        # Fully connected layer\n","        x = self.fc(x)\n","        return x\n"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1701215905104,"user":{"displayName":"Georgia Chen","userId":"17843376469141931868"},"user_tz":300},"id":"9B_Lh3ZUnhXd"},"outputs":[],"source":["image_folder = '/content/drive/MyDrive/Colab Notebooks/test_img'\n","label_folder = '/content/drive/MyDrive/Colab Notebooks/entities'\n","csv_file = '/content/drive/MyDrive/Colab Notebooks/output.csv'\n","\n","# data = []\n","\n","# # Iterate through each image in the image folder\n","# for image_file in os.listdir(image_folder):\n","#     if image_file.endswith(('.png', '.jpg', '.jpeg')):  # Add other image formats if needed\n","#         image_path = os.path.join(image_folder, image_file)\n","\n","#         # Construct the corresponding label file path\n","#         label_file = os.path.splitext(image_file)[0] + '.txt'  # Assuming text files have .txt extension\n","#         label_path = os.path.join(label_folder, label_file)\n","\n","#         # Read the label content\n","#         if os.path.exists(label_path):\n","#             with open(label_path, 'r') as file:\n","#                 label = ground_truth(label_path)\n","#                 data.append([image_path, label])\n","\n","# # Create a DataFrame and save as CSV\n","# df = pd.DataFrame(data, columns=['image_path', 'label'])\n","# df.to_csv(csv_file, index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["image_folder = ''"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NL9gARB9nkre","outputId":"759f3f56-f120-4e20-8ade-c28a9df023a0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape after CNN: torch.Size([16, 128, 16, 512])\n","Shape after flattening: torch.Size([16, 2048, 512])\n"]}],"source":["import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision import transforms\n","import gc\n","\n","# Custom dataset class\n","class ReceiptsDataset(Dataset):\n","    def __init__(self, csv_file, transform=None, fixed_height=128, max_width=4096):\n","        self.dataframe = pd.read_csv(csv_file)\n","        self.transform = transform\n","        self.fixed_height = fixed_height\n","        self.max_width = max_width\n","\n","    def __len__(self):\n","        return len(self.dataframe)\n","\n","    def __getitem__(self, idx):\n","        img_name = self.dataframe.iloc[idx, 0]\n","        image = Image.open(img_name).convert('L')\n","        image = self.resize_and_pad(image)\n","        label = self.dataframe.iloc[idx, 1]\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, label\n","\n","    def resize_and_pad(self, image):\n","        aspect_ratio = image.width / image.height\n","        new_width = int(self.fixed_height * aspect_ratio)\n","        resized_image = image.resize((new_width, self.fixed_height), Image.LANCZOS)\n","\n","        if new_width < self.max_width:\n","            padding_left = (self.max_width - new_width) // 2\n","            padding_right = self.max_width - new_width - padding_left\n","            padding_top = padding_bottom = 0\n","            resized_image = ImageOps.expand(resized_image, border=(padding_left, padding_top, padding_right, padding_bottom), fill=0)\n","\n","        return resized_image\n","\n","# Transformations\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","])\n","\n","# Load dataset\n","train_dataset = ReceiptsDataset(csv_file='/content/drive/MyDrive/Colab Notebooks/output.csv', transform=transform)\n","train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","\n","# Model, loss, and optimizer\n","num_classes = 100  # Set this to your number of classes (characters)\n","model = OCRModel(num_classes)\n","ctc_loss = nn.CTCLoss(blank=0)  # Assuming 0 is the index for the blank character\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# Training loop\n","unique_chars = set()\n","for _, label in train_dataset:\n","    unique_chars.update(label)\n","\n","# Add a special character for CTC blank token, if it's not already included\n","unique_chars.add('')\n","\n","# Create char_to_int mapping\n","char_to_int = {char: i for i, char in enumerate(sorted(unique_chars))}\n","# Int to char\n","#int_to_char = {i: char for char, i in char_to_int.items()}\n","\n","num_epochs = 10  # Set the number of epochs\n","for epoch in range(num_epochs):\n","    model.train()  # Set the model to training mode\n","    total_loss = 0\n","\n","    for batch_idx, (images, labels) in enumerate(train_loader):\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        outputs = model(images)  # outputs shape after model: [batch_size, channels * height, width]\n","\n","        # Correctly permute and reshape outputs for CTC\n","        outputs = outputs.permute(2, 0, 1)  # now shape: [width, batch_size, channels * height]\n","        outputs = outputs.reshape(512, 16, num_classes)\n","\n","        # Flatten all labels into a single list and calculate label_lengths\n","        flat_labels = []\n","        label_lengths = []\n","        for label in labels:\n","            char_indices = [char_to_int[char] for char in label]  # Convert each char to its index\n","            flat_labels.extend(char_indices)\n","            label_lengths.append(len(char_indices))\n","\n","        # Convert lists to tensors\n","        flat_labels = torch.IntTensor(flat_labels)\n","        label_lengths = torch.IntTensor(label_lengths)\n","        input_lengths = torch.full(size=(32,), fill_value=620, dtype=torch.int32)\n","\n","        # Check if label_lengths size matches batch size\n","        if label_lengths.size(0) != images.size(0):\n","            raise ValueError(f\"Size mismatch: label_lengths ({label_lengths.size(0)}) vs batch_size ({images.size(0)})\")\n","\n","        # Calculate the input lengths for CTC Loss\n","        input_lengths = torch.full(size=(outputs.size(1),), fill_value=outputs.size(0), dtype=torch.int32)\n","\n","        # Calculate CTC Loss\n","        loss = ctc_loss(outputs.log_softmax(2), flat_labels, input_lengths, label_lengths)\n","        total_loss += loss.item()\n","\n","        # Backward pass: Compute gradient of the loss with respect to model parameters\n","        loss.backward()\n","\n","        # Perform a single optimization step (parameter update)\n","        optimizer.step()\n","        del images, outputs, flat_labels, label_lengths, input_lengths, loss\n","        gc.collect()\n","\n","        if batch_idx % 10 == 0:  # Print loss every 10 batches\n","            print(f\"Epoch: {epoch+1}, Batch: {batch_idx+1}, Loss: {loss.item()}\")\n","\n","    print(f\"Epoch: {epoch+1}, Average Loss: {total_loss / len(train_loader)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VhiFPYhDx7ya"},"outputs":[],"source":["    # Function to convert model output to text\n","def decode_ctc_output(output, char_map):\n","    # Applying softmax to get probabilities\n","    probs = torch.softmax(output, 2)\n","    probs = probs.permute(1, 0, 2)  # Reshape to batch first\n","    max_probs, indices = torch.max(probs, 2)\n","\n","    # Convert indices to characters\n","    texts = []\n","    for i in range(indices.size(0)):\n","        text = ''.join([char_map[index] for index in indices[i] if index != 0])  # 0 is the CTC blank character\n","        texts.append(text)\n","\n","    return texts\n","\n","# Function to process an image and predict text\n","def predict_text(model, image_path, char_map):\n","    # Load and transform the image\n","    image = Image.open(image_path).convert('L')\n","    transform = transforms.Compose([transforms.ToTensor()])\n","    image = transform(image)\n","    image = image.unsqueeze(0)  # Add batch dimension\n","\n","    # Forward pass\n","    model.eval()\n","    with torch.no_grad():\n","        output = model(image)\n","\n","    # Decode the output to text\n","    predicted_text = decode_ctc_output(output, char_map)\n","\n","    return predicted_text\n","\n","int_to_char = {i: char for char, i in char_to_int.items()}\n","\n","# Example usage\n","image_path = '/content/drive/MyDrive/Colab Notebooks/test_img/X00016469670/jpg'\n","predicted_text = predict_text(model, image_path, int_to_char)\n","print(f\"Predicted text: {predicted_text[0]}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kh9q4ujUnoLc"},"outputs":[],"source":["def test_model_on_images(model, image_folder, char_map, label_folder):\n","    for image_file in os.listdir(image_folder):\n","        if image_file.endswith(('.png', '.jpg', '.jpeg')):\n","            image_path = os.path.join(image_folder, image_file)\n","            predicted_text = predict_text(model, image_path, char_map)[0]\n","\n","            # Construct the corresponding label file path\n","            label_file = os.path.splitext(image_file)[0] + '.txt'\n","            label_path = os.path.join(label_folder, label_file)\n","\n","            # Read the ground truth label\n","            if os.path.exists(label_path):\n","                ground_truth_label = ' '.join(ground_truth(label_path))  # Joining the ground truth elements to form a sentence\n","\n","                # Calculate WER\n","                error_rate = wer(ground_truth_label, predicted_text)\n","\n","                # Print results\n","                print(f\"Image: {image_file}, Predicted text: {predicted_text}, WER: {error_rate:.2f}\")\n","\n","# Test the model\n","model = OCRModel\n","test_image_folder = '/content/drive/MyDrive/test/img'\n","test_model_on_images(model, test_image_folder, int_to_char)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPBMfjUwh2MHb+DZBDZxWtJ","mount_file_id":"1Ldw0e5TVxPapdDbn7QU04rHDDl6Tu9rE","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
